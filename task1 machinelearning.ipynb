{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习绪论\n",
    "主要参考了西瓜书和统计学习方法论，有不对的地方希望大家指正"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.机器学习的基本术语\n",
    "### 1.1 什么是机器学习\n",
    "机器学习是一门致力于研究如何通过计算的手段，利用经验改善系统自身的性能的学科。人的“经验”通常以“数据”的形式存在，让计算机来学习这些经验数据，生成一个算法模型，在面对新的情况中，计算机便能作出有效的判断，这便是机器学习。\n",
    "\n",
    "Mitchell给出了一个形式化的定义，假设：\n",
    "* T：计算机要实现的任务类T\n",
    "* P：计算机程序在某任务类T的性能\n",
    "* E：经验，即历史数据\n",
    "\n",
    "若该计算机程序通过利用经验E在任务T上获得了性能P的改善，则称该程序对E进行了学习。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2基本术语\n",
    "假设我们收集了一批西瓜的数据，例如：（色泽=青绿;根蒂=蜷缩;敲声=浊响)， (色泽=乌黑;根蒂=稍蜷;敲声=沉闷)， (色泽=浅自;根蒂=硬挺;敲声=清脆)……每对括号内是一个西瓜的记录，定义：\n",
    "\n",
    "* 数据集：收集到的这组记录的集合\n",
    "\n",
    "* 样本or示例：其中的每条记录\n",
    "\n",
    "* 属性or特征：反映事件或者对象在某方面的表现或者性质的事项，如“色泽”\n",
    "\n",
    "* 属性空间： 属性张成的空间\n",
    "\n",
    "* 特征向量：在属性空间内，每个示例都能找到一个坐标位置，每个点对应一个坐标向量，因此一个示例为一个“特征向量”\n",
    "\n",
    "* 维数：样本的特征数\n",
    "\n",
    "* 学习or训练：从数据中得到模型的过程\n",
    "\n",
    "* 训练数据：训练过程中使用的数据\n",
    "\n",
    "* 训练集：训练样本组成的集合\n",
    "\n",
    "* 假设：学得的模型对应了关于数据的某种潜在的规律\n",
    "\n",
    "* 真相or真实（ground truth）：这种潜在规律自身\n",
    "\n",
    "* 学习器： 学习过程中得到的模型\n",
    "\n",
    "* 预测：西瓜的例子中，通过学习西瓜的特征数据，训练出一个决策模型，来判断一个新的西瓜是否是好瓜。可以得知我们预测的是：西瓜是好是坏，即好瓜与差瓜两种，是离散值。同样地，也有通过历年的人口数据，来预测未来的人口数量，人口数量则是连续值。定义：\n",
    "\n",
    "   **预测值为离散值的问题为：分类（classification）**\n",
    "\n",
    "   **预测值为连续值的问题为：回归（regression）**\n",
    "\n",
    "* 二分类任务（binary classification）：只涉及两个类别，通常称一个为正类（positive class），一个为反类（negative class）\n",
    "\n",
    "* 多分类任务（multi-class classification）：涉及多个类别\n",
    "\n",
    "* 聚类（clustering）：即把训练集中的西瓜分成若干个组，每个组称为一个簇（cluster）；这些自动形成的簇可能对应一些潜在的概念划分，例如浅色瓜，本地瓜等\n",
    "\n",
    "* 根据训练数据是否拥有标记信息，学习任务大致划分为两类：监督学习（supervised learning）无监督学习（unsupervised learning），分类和回归是前者的代表，而聚类是后者的代表\n",
    "\n",
    "* 泛化：学得的模型适用于新样本的能力\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3假设空间\n",
    "学习过程就是一个在所有假设（hypothesis）组成的空间中进行搜索的过程，搜索目标是找到与训练集“匹配”（fit）的假设，即能够将训练集中的瓜判断正确的假设。\n",
    "可以有许多策略对假设空间进行搜索，例如自顶向下，从一般到特殊，或者自底向上、从特殊到一般，搜索过程中可以不断删除与正例不一致的假设、和（或）反例一致的假设。最终将会获得与训练集一致（即对所有训练样本能够进行正确判断）的假设，这就是结果。\n",
    "版本空间（version space）：有多个假设与训练集一致，即存在着一个与训练集一致的“假设集合”。\n",
    "\n",
    "### 1.4归纳偏好\n",
    "归纳偏好（inductive bias）：机器学习算法在学习过程中对某种类型假设的偏好。通过机器学习得到的模型对应了空间中的一个假设，版本空间的多个假设可能会产生不同的输出，但我们就一个具体的学习算法而言，必须要产生一个模型，归纳偏好在这起了关键作用。\n",
    "\n",
    "归纳偏好可看作学习算法自身在一个可能很庞大的假设空间中对假设进行选择的启发式或‘价值观’。\n",
    "\n",
    "**奥卡姆剃刀**（Occam’s razor）：若有多个假设与观察一致，则选最简单的那个\n",
    "\n",
    "**没有免费的午餐定理**（No Free Lunch Theorem）:总误差与学习算法无关! NFL（No Free Lunch Theorem）没有免费的午餐定理：\n",
    "1）所有“问题”出现的机会相同、或所有问题同等重要\n",
    "2）假设真实的目标函数 f 均匀分布。\n",
    "**NFL定理告诉我们必须针对具体的学习问题来谈算法的优劣。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、机器学习分类\n",
    "### 2.1监督学习\n",
    "监督学习是指利用一组已知类别的样本调整分类器的参数，使其达到所要求性能的过程，也称为监督训练或有教师学习。在监督学习的过程中会提供对错指示，通过不断地重复训练，使其找到给定的训练数据集中的某种模式或规律，当新的数据到来时，可以根据这个函数预测结果。监督学习的训练集要求包括输入和输出，主要应用于分类和预测。\n",
    "### 2.2非监督学习\n",
    "与监督学习不同，在非监督学习中，无须对数据集进行标记，即没有输出。其需要从数据集中发现隐含的某种结构，从而获得样本数据的结构特征，判断哪些数据比较相似。因此，非监督学习目标不是告诉计算机怎么做，而是让它去学习怎样做事情。\n",
    "\n",
    "### 2.3半监督学习\n",
    "半监督学习是监督学习和非监督学习的结合，其在训练阶段使用的是未标记的数据和已标记的数据，不仅要学习属性之间的结构关系，也要输出分类模型进行预测。\n",
    "\n",
    "### 2.4强化学习\n",
    "强化学习（Reinforcement Learning, RL），又称再励学习、评价学习或增强学习，是机器学习的范式和方法论之一，用于描述和解决智能体（agent）在与环境的交互过程中通过学习策略以达成回报最大化或实现特定目标的问题."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.模型的评估与选择\n",
    "### 3.1误差与过拟合\n",
    "我们将学习器对样本的实际预测结果与样本的真实值之间的差异成为：误差（error）。定义：\n",
    "\n",
    "* 在训练集上的误差称为训练误差（training error）或经验误差（empirical error）\n",
    "* 在测试集上的误差称为测试误差（test error）\n",
    "* 学习器在所有新样本上的误差称为泛化误差（generalization error）\n",
    "\n",
    "我们希望得到的是在新样本上表现得很好的学习器，即泛化误差小的学习器。因此，我们应该让学习器尽可能地从训练集中学出普适性的**“一般特征”**，这样在遇到新样本时才能做出正确的判别。然而，当学习器把训练集学得“太好”的时候，即把一些训练样本的自身特点当做了普遍特征；同时也有学习能力不足的情况，即训练集的基本特征都没有学习出来。我们定义：\n",
    "\n",
    "* 过拟合（overfitting）：学习能力过强，以至于把训练样本所包含的不太一般的特性都学到了。\n",
    "* 欠拟合（underfitting）：学习能太差，训练样本的一般性质尚未学好。\n",
    "\n",
    "在过拟合问题中，训练误差十分小，但测试误差大；在欠拟合问题中，训练误差和测试误差都比较大。目前，欠拟合问题比较容易克服，例如增加迭代次数等，但过拟合问题还没有十分好的解决方案，过拟合是机器学习面临的关键障碍。\n",
    "### 3.2训练集与测试集的划分方法\n",
    "我们希望用一个“测试集”的“测试误差”来作为“泛化误差”的近似，因此我们需要对初始数据集进行有效划分，划分出互斥的“训练集”和“测试集”。下面介绍几种常用的划分方法:\n",
    "* **留出法** : 将数据集D划分为两个互斥的集合，一个作为训练集S，一个作为测试集T，满足D=S∪T且S∩T=∅\n",
    "* **交叉验证法** : 将数据集D划分为k个大小相同的互斥子集，满足D=D1∪D2∪...∪Dk，Di∩Dj=∅（i≠j），同样地尽可能保持数据分布的一致性，即采用分层抽样的方法获得这些子集。交叉验证法的思想是：每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集，这样就有K种训练集/测试集划分的情况，从而可进行k次训练和测试，最终返回k次测试结果的均值。交叉验证法也称“k折交叉验证”，k最常用的取值是10。\n",
    "* **自助法** ： 给定包含m个样本的数据集D，有放回的从D中选一个样本，拷贝到D’，执行m次后，得到m个样本的D’\n",
    "\n",
    "### 3.3 调参\n",
    "\n",
    "大多数学习算法都有些参数(parameter) 需要设定，参数配置不同，学得模型的性能往往有显著差别，这就是通常所说的\"参数调节\"或简称\"调参\" (parameter tuning)。\n",
    "\n",
    "学习算法的很多参数是在实数范围内取值，因此，对每种参数取值都训练出模型来是不可行的。常用的做法是：对每个参数选定一个范围和步长λ，这样使得学习的过程变得可行。例如：假定算法有3 个参数，每个参数仅考虑5 个候选值，这样对每一组训练/测试集就有555= 125 个模型需考察，由此可见：拿下一个参数（即经验值）对于算法人员来说是有多么的happy。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
